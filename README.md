# ğŸ‘ï¸ Real Time Vision System - Beta v0.1.1
> An AI-powered real-time visual analysis system using computer vision and natural language processing for scene understanding and intelligent narration.

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![CUDA](https://img.shields.io/badge/CUDA-11.8-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
![Tests](https://github.com/taseebali/real-time-vision-system/actions/workflows/tests.yml/badge.svg)
![Docker](https://github.com/taseebali/real-time-vision-system/actions/workflows/docker.yml/badge.svg)

## ğŸŒŸ Features

### âœ… Currently Working
- **ğŸš€ High-Performance Processing** - 8-9 FPS real-time processing (10x faster than v0.0.1)
- **ğŸ¯ Smart Narration** - Intelligent throttling prevents spam (5-second intervals, requires meaningful changes)
- **ğŸ–¼ï¸ Selective Scene Captioning** - AI descriptions only when needed (80% overhead reduction)
- **ğŸ“Š Real-time Performance Monitoring** - FPS, latency, GPU/CPU usage tracking
- **ğŸ‘ï¸ Real-time Object Detection** - YOLOv8-medium detecting 80+ object classes in 35-40ms
- **ğŸ“ Spatial Awareness** - Distance estimation and positional tracking (left/right/center, above/below)
- **ğŸ’¬ Natural Language Narration** - Contextual scene descriptions with spatial relationships
- **ğŸ”Š Text-to-Speech** - Audio feedback using Google Text-to-Speech (gTTS)
- **âš¡ GPU Acceleration** - CUDA-optimized with mixed precision (FP16) for maximum speed
- **ğŸ“± Phone Camera Integration** - Connect via IP Webcam app (WiFi)
- **ğŸ›ï¸ Configurable Modes** - Toggle captioning and display for different performance profiles

### ğŸš§ In Development
- Text Recognition (OCR) - Disabled due to compatibility issues (planned fix)
- Voice Commands - Model ready but not integrated yet
- TensorRT Optimization - Target 10+ FPS on laptop
- Mobile App Development - Android/iOS deployment

## ğŸ“‹ Table of Contents
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Performance](#-performance)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)


**What it does:**
1. Connects to your phone camera via WiFi
2. Detects objects in real-time (8-9 FPS)
3. Generates AI scene descriptions when needed
4. Estimates distances and positions
5. Speaks intelligent, non-repetitive descriptions

**Example Output:**
> "I can see a laptop in the center. a close up of a laptop with a mouse on it"  
> [5 seconds later, after objects change]  
> "I can see a mouse on the right. a computer mouse sitting on top of a wooden table"

## ğŸ’» Prerequisites

### Hardware Requirements
- **GPU**: NVIDIA GPU with CUDA support (Tested on RTX 3050, 4GB+ VRAM recommended)
- **RAM**: 8GB minimum, 16GB recommended
- **Storage**: 5GB for models and dependencies

### Software Requirements
- **OS**: Windows 10/11, Linux (Ubuntu 20.04+), or macOS
- **Python**: 3.11 or higher
- **CUDA**: 11.8 (for GPU acceleration)
- **cuDNN**: Compatible with CUDA 11.8

### Mobile Requirements
> Note: Currently, it uses camera from the phone using IP Webcam. The codes can be changed to implement computer webcams. Create a request for that specific feature.
- Android phone with [IP Webcam](https://play.google.com/store/apps/details?id=com.pas.webcam) app
- Same WiFi network as computer

## ğŸš€ Installation

Choose your preferred installation method:

### Method 1: Docker (Easiest - Recommended for Linux)

**Prerequisites**: Docker with NVIDIA Container Toolkit

```bash
# Clone the repository
git clone https://github.com/taseebali/real-time-vision-system.git
cd real-time-vision-system

# Build and run with Docker Compose
docker-compose -f docker/docker-compose.yml up --build

# Or use the convenience script
# Linux/Mac
chmod +x docker/docker-build.sh
./docker/docker-build.sh build
./docker/docker-build.sh run 192.168.1.100

# Windows
docker\docker-build.bat build
docker\docker-build.bat run 192.168.1.100
```

ğŸ‘‰ **See [Docker README](docker/README.md) for detailed instructions**

### Method 2: Quick Setup Script

```bash
# 1. Clone the repository
git clone https://github.com/taseebali/real-time-vision-system.git
cd real-time-vision-system

# 2. Run the setup script
# Windows
setup.bat

# Linux/Mac
chmod +x setup.sh
./setup.sh
```

### Method 3: Manual Installation

```bash
# 1. Clone the repository
git clone https://github.com/taseebali/real-time-vision-system.git
cd real-time-vision-system

# 2. Create virtual environment
python -m venv .venv

# 3. Activate virtual environment
# Windows
.\.venv\Scripts\activate
# Linux/Mac
source .venv/bin/activate

# 4. Install PyTorch with CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 5. Install dependencies
pip install -r requirements.txt

# 6. Verify installation
python -m tests.test_cuda
```

## âš¡ Quick Start

### Step 1: Setup Video Source

1. Install **IP Webcam** app on your Android phone (or use any IP camera/HTTP video source)
2. Open the app and start the server
3. Note the IP address shown (e.g., `192.168.1.100`)
4. Make sure your device and computer are on the same network

### Step 2: Run the System

```bash
# Activate virtual environment (if not already active)
.\.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac

# Run with your camera's IP address
python run.py
# When prompted, enter your camera's IP address (e.g., 192.168.1.100)
```

### Step 3: Use the Application

- Point your camera at scenes for analysis
- Listen to the audio descriptions
- Press `Q` to quit the display window
- Press `Ctrl+C` to stop the application

## ğŸ“– Usage

### Basic Usage

```python
from src.core.optimized_assistant import OptimizedAssistant

# Create system with display and scene captioning
vision_system = OptimizedAssistant(
    show_display=True,
    camera_ip="192.168.1.100",  # Your camera's IP
    enable_captioning=True      # Enable AI scene descriptions
)

# Start the system
vision_system.start()
```

### Advanced Usage

```python
# Disable visual display (audio only) or disable scene captioning
vision_system = OptimizedAssistant(
    show_display=False, 
    camera_ip="192.168.1.100",
    enable_captioning=False  # Disable captioning for faster processing
)

# Use with custom settings
from src.services.detection.object_detector import ObjectDetector

detector = ObjectDetector(
    model_name='yolov8m.pt',
    confidence_threshold=0.30
)
```

### Command Line Options

```bash
# Run with specific camera IP
python run.py 192.168.1.100

# Run tests
python -m pytest tests/

# Check GPU status
python -m tests.test_cuda

# Test with sample image
python -m tests.run_image_test
```

## Running Tests

1. Run all tests:
```bash
python -m pytest tests/
```

2. Run specific test modules:
```bash
# GPU/CUDA tests
python -m tests.test_cuda

# Image processing tests
python -m tests.run_image_test

# NVIDIA driver tests
python -m tests.test_nvidia
```

## Features

- Real-time object detection using YOLOv8
- Text recognition using EasyOCR
- Scene description using BLIP
- Natural language narration
- Text-to-speech output
- GPU acceleration for all AI models
- Comprehensive testing and logging

## ğŸ“ Project Structure

```
real-time-vision-system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ README.md                    # Core module documentation
â”‚   â”‚   â””â”€â”€ optimized_assistant.py       # Main system orchestrator
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ detection/
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md                # Detection services docs
â”‚   â”‚   â”‚   â”œâ”€â”€ object_detector.py       # YOLOv8 object detection
â”‚   â”‚   â”‚   â”œâ”€â”€ text_detector.py         # EasyOCR text recognition (disabled)
â”‚   â”‚   â”‚   â””â”€â”€ caption_generator.py     # BLIP image captioning (disabled)
â”‚   â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md                # Audio services docs
â”‚   â”‚   â”‚   â”œâ”€â”€ text_to_speech.py        # gTTS text-to-speech
â”‚   â”‚   â”‚   â””â”€â”€ speech_recognizer.py     # Whisper (not implemented)
â”‚   â”‚   â”œâ”€â”€ README.md                    # Services overview
â”‚   â”‚   â”œâ”€â”€ camera_service.py            # IP Webcam integration
â”‚   â”‚   â””â”€â”€ narration_service.py         # Natural language generation
â”‚   â””â”€â”€ main.py                          # Application entry point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data/                       # Sample test images
â”‚   â”œâ”€â”€ test_output/                     # Test results
â”‚   â”œâ”€â”€ utils/logging/                   # Test logging utilities
â”‚   â”œâ”€â”€ README.md                        # Testing documentation
â”‚   â”œâ”€â”€ test_cuda.py                     # GPU/CUDA verification
â”‚   â”œâ”€â”€ test_image_processing.py         # Integration tests
â”‚   â””â”€â”€ test_nvidia.py                   # NVIDIA driver tests
â”œâ”€â”€ run.py                               # Main entry point
â”œâ”€â”€ requirements.txt                     # Python dependencies
â”œâ”€â”€ .gitignore                           # Git ignore rules
â”œâ”€â”€ LICENSE                              # MIT License
â””â”€â”€ README.md                            # This file
```

## âš¡ Performance

### Current Metrics (Optimized - v0.1.0)

**RTX 3050 Laptop GPU (4GB VRAM)**

#### Processing Speed
- **Overall FPS**: 8-9 FPS (10x faster than v0.0.1)
- **Object Detection**: 35-40ms per frame
- **Scene Captioning**: 350-450ms (only when triggered)
- **Narration Generation**: <1ms
- **Total Response Time**: ~100ms from scene change to narration

#### Resource Usage
- **GPU Memory**: 0.7GB allocated, 1.1GB reserved (down from 3-4GB)
- **CPU Usage**: 8-12% average
- **RAM Usage**: System dependent

#### Optimization Results
- **Frame Skip Ratio**: ~240% (processes 1 in 3 frames efficiently)
- **Captioning Overhead**: Reduced from 100% to ~20% via selective triggering
- **Narration Rate**: ~7-8 meaningful narrations per 38 seconds (vs 96 spam before)

### Performance Modes

#### Maximum Speed (Headless + No Captioning)
```python
assistant = OptimizedAssistant(
    show_display=False,
    enable_captioning=False
)
# Expected: 10+ FPS
```

#### Balanced (Default)
```python
assistant = OptimizedAssistant(
    show_display=True,
    enable_captioning=True
)
# Expected: 8-9 FPS
```

#### Full Features (Display + Captioning)
```python
assistant = OptimizedAssistant(
    show_display=True,
    enable_captioning=True
)
# Expected: 8-9 FPS (optimized captioning keeps speed high)
```

### Optimization Tips
1. **Disable captioning** for maximum speed: `enable_captioning=False`
2. **Adjust frame skip**: `assistant.frame_skip = 5` (process every 5th frame)
3. **Increase narration interval**: `assistant.narration_interval = 10` (narrate every 10s)
4. **Disable visual display**: `show_display=False` for headless operation
5. **Use smaller model** (future): Switch to `yolov8n.pt` for even faster processing

### Real-Time Monitoring

The system provides live performance metrics:
```
Processing: 8.79 FPS | Avg: 37.3ms

============================================================
PERFORMANCE REPORT (Every 10 seconds)
============================================================

Timing (ms):
  object_detection         :   35.1 (min:  29.1, max:  39.8)
  scene_captioning         :  415.1 (min: 345.1, max: 571.4)
  narration_generation     :    0.3 (min:   0.0, max:   1.5)
  
Counters:
  frames_processed         : 269
  captions_generated       : 6
  narrations_generated     : 81
  
Rates:
  Processing FPS:           7.88
```

## ğŸ› Troubleshooting

### Installation Issues

**Problem**: `CUDA not available`
```bash
# Solution: Verify PyTorch CUDA installation
python -c "import torch; print(torch.cuda.is_available())"
python -c "import torch; print(torch.version.cuda)"

# Reinstall PyTorch with CUDA if needed
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**Problem**: `ModuleNotFoundError`
```bash
# Solution: Ensure virtual environment is activated and dependencies installed
pip install -r requirements.txt
```

### Runtime Issues

**Problem**: Cannot connect to phone camera
```bash
# Solution:
# 1. Check both devices are on same WiFi
# 2. Verify IP address in IP Webcam app
# 3. Try pinging the phone: ping 192.168.1.100
# 4. Check firewall settings
```

**Problem**: Slow performance / Low FPS
```bash
# Solution:
# 1. Check GPU usage: nvidia-smi
# 2. Reduce camera quality in IP Webcam settings
# 3. Use smaller model in object_detector.py
# 4. Close other GPU-intensive applications
```

**Problem**: Audio not working
```bash
# Solution:
# 1. Check internet connection (gTTS requires internet)
# 2. Verify speakers/headphones are connected
# 3. Check Windows audio settings
```

### Testing

```bash
# Verify GPU setup
python -m tests.test_cuda

# Test object detection
python -m tests.run_image_test

# Check NVIDIA drivers
python -m tests.test_nvidia

# Run all tests
python -m pytest tests/ -v
```

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. **Commit** your changes (`git commit -m 'Add some AmazingFeature'`)
4. **Push** to the branch (`git push origin feature/AmazingFeature`)
5. **Open** a Pull Request

### Development Guidelines
- Follow PEP 8 style guide
- Add tests for new features
- Update documentation
- Keep commits atomic and descriptive

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **YOLOv8** by Ultralytics
- **EasyOCR** by JaidedAI
- **BLIP** by Salesforce Research
- **gTTS** by Pierre Nicolas Durette
- **IP Webcam** by Pavel Khlebovich

## ğŸ“§ Contact

**Author**: Taseeb Ali
**GitHub**: [@taseebali](https://github.com/taseebali)
**Repository**: [real-time-vision-system](https://github.com/taseebali/real-time-vision-system)

## ğŸ—ºï¸ Roadmap

### v0.2.0 (Next Release)
- [ ] Fix text detection (OCR)
- [ ] Implement voice commands
- [ ] Add offline mode
- [ ] Improve response time (<1 second)
- [ ] Mobile app version

### v0.3.0 (Future)
- [ ] Navigation assistance
- [ ] Obstacle detection
- [ ] Face recognition
- [ ] Custom wake words
- [ ] Cloud deployment

---

**Note**: This is a beta version. Please report issues on the [GitHub Issues](https://github.com/taseebali/real-time-vision-system/issues) page.
